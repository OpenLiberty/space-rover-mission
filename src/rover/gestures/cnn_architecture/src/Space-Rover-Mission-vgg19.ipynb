{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwKt6al854oH"
      },
      "source": [
        "### Importing images and files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_OtMOvdmU6e6"
      },
      "outputs": [],
      "source": [
        "#Import all the important libraries\n",
        "import os\n",
        "import sys\n",
        "import tkinter as tk\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import cv2\n",
        "import cvzone\n",
        "import mediapipe as mp\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from cvzone.HandTrackingModule import HandDetector\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras.models import Sequential, load_model\n",
        "from keras import backend as K\n",
        "from keras.layers.core import Dense, Flatten\n",
        "from keras import optimizers\n",
        "#from keras.optimizers import Adam\n",
        "from keras.metrics import categorical_crossentropy\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "#from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.convolutional import *\n",
        "\n",
        "import itertools\n",
        "%matplotlib inline\n",
        "\n",
        "from math import floor\n",
        "from PIL import Image, ImageTk\n",
        "import serial\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDUUbKcP5_bC"
      },
      "source": [
        "### Taking pictures for the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAHNdtsHQOeg"
      },
      "outputs": [],
      "source": [
        "# Start video stream from your computer camera \n",
        "capture = cv2.VideoCapture(0) \n",
        "\n",
        "# Capture continuous video\n",
        "i = 1 \n",
        "while True:\n",
        "    ret, image = capture.read() # read from the video\n",
        "    image = cv2.flip(image,1) # flip your video\n",
        "    \n",
        "    cv2.rectangle(image,(360,60),(590,290),(0,255,0),5) # draw a green square, the location of two vertexs are (360,60) and (590,290)\n",
        "    # so the size of picture is \n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX # define your font\n",
        "    cv2.putText(image,'Put your RIGHT hand in the square',(255,40),font,0.7,(0,255,0),1,cv2.LINE_AA) # write green text at start at location (255,40)\n",
        "    print(image.shape)\n",
        "    cv2.imshow('Taking data pictures',image)  # display your video on your computer as name 'taking picture'\n",
        "    k = cv2.waitKey(1) # get input from keyboard\n",
        "    if k == ord('c'): #press 'c' key to capture photo, and increment 1\n",
        "        i+=1\n",
        "        print(i) # print out the counter value\n",
        "        \n",
        "        cv2.imwrite('data/train/0/imag'+str(i)+'.jpg',frame[63:287,363:587]) # save your picture with size (224,224) to some location in the same folder\n",
        "        #remember change to corresponding directory\n",
        "          \n",
        "    if k ==27: # press 'Esc' key to exit \n",
        "        break # exit while loop\n",
        "        \n",
        "#========================================\n",
        "\n",
        "cv2.destroyAllWindows() # close down window\n",
        "capture.release() # disable your camera        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5UALEbSEhPA"
      },
      "source": [
        "### Data Processing and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "u0M4Kc-JFtPE"
      },
      "outputs": [],
      "source": [
        "# Directories for all the data\n",
        "\n",
        "trainingDirectory = '../SpaceRoverProject/Training'\n",
        "testingDirectory = '../SpaceRoverProject/Testing'\n",
        "validationDirectory = '../SpaceRoverProject/Validation'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QekiKOXfF0IE",
        "outputId": "6ea15967-6f76-4a95-a54f-80aeb9a84e01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 710 images belonging to 5 classes.\n",
            "Found 245 images belonging to 5 classes.\n",
            "Found 242 images belonging to 5 classes.\n"
          ]
        }
      ],
      "source": [
        "# Generate batches of tensor image data with real-time data augmentation.\n",
        "\n",
        "train_batches = ImageDataGenerator().flow_from_directory(trainingDirectory,target_size=(224,224),classes=['Forward','Left','Reverse','Right','Stop'],batch_size=8) \n",
        "# target_size: the size of pictures | classes: the file name of your different classes \n",
        "test_batches = ImageDataGenerator().flow_from_directory(testingDirectory,target_size=(224,224),classes=['Forward','Left','Reverse','Right','Stop'],batch_size=5)\n",
        "valid_batches = ImageDataGenerator().flow_from_directory(validationDirectory,target_size=(224,224),classes=['Forward','Left','Reverse','Right','Stop'],batch_size=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "l_UmjyrQHzcu"
      },
      "outputs": [],
      "source": [
        "# Define plots function to show sample images from data\n",
        "\n",
        "def plots(ims,figsize=(12,6),rows=1,interp=False,titles=None):\n",
        "    if type(ims[0]) is np.ndarray:\n",
        "        ims = np.array(ims).astype(np.uint8)\n",
        "        if (ims.shape[-1]!=3):\n",
        "            ims = ims.transpose((0,2,3,1))\n",
        "    f = plt.figure(figsize=figsize)\n",
        "    cols = len(ims)//rows if len(ims) % 2 ==0 else len(ims)//rows + 1\n",
        "    for i in range(len(ims)):\n",
        "        sp = f.add_subplot(rows,cols,i+1)\n",
        "        sp.axis('Off')\n",
        "        if titles is not None:\n",
        "            sp.set_title(titles[i],fontsize=12)\n",
        "        plt.imshow(ims[i],interpolation=None if interp else 'none')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XQzu5MNlIAf5"
      },
      "outputs": [],
      "source": [
        "imgs, labels = next(train_batches) # read the image and corresponding labels which indicate the class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "tJsPdF7hMiG6",
        "outputId": "0f286c29-b494-47ba-a3b8-466f6a3f29fc"
      },
      "outputs": [],
      "source": [
        "plots(imgs,titles=labels) # the labels are in 'one-hot' format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTAe8584M5Bb"
      },
      "source": [
        "### Loading vgg19 pretrained model from Keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khGlk8FmNAWC",
        "outputId": "f889eed8-ed9b-49cc-b90a-5ef413cfbb89"
      },
      "outputs": [],
      "source": [
        "vgg19_model = tf.keras.applications.vgg19.VGG19(include_top=True,weights='imagenet')\n",
        "# automatically download vgg19 model from the website below, which is about 500 MB,\n",
        "# which may takes 15 mins\n",
        "# once downloaded, you don't need to download again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6h9hGlsQSmRR",
        "outputId": "26377581-cb8a-45d5-f474-951615f6d053"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"vgg19\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv4 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv4 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv4 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 25088)             0         \n",
            "                                                                 \n",
            " fc1 (Dense)                 (None, 4096)              102764544 \n",
            "                                                                 \n",
            " fc2 (Dense)                 (None, 4096)              16781312  \n",
            "                                                                 \n",
            " predictions (Dense)         (None, 1000)              4097000   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 143,667,240\n",
            "Trainable params: 143,667,240\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Show the model structure of vgg19\n",
        "\n",
        "vgg19_model.summary() # contain total 19 layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPHaZwpeSqUg",
        "outputId": "880563fa-f6ea-456c-cf3f-54e9fe90f8df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "keras.engine.functional.Functional"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(vgg19_model) # the type of the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzoDVmSTTAOI"
      },
      "source": [
        "### Configuring the vgg19 Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1dbYeQ0St7z",
        "outputId": "77015317-bbee-4cee-eac6-f0cf272f13a0"
      },
      "outputs": [],
      "source": [
        "# Changing the model type from \"model\" to \"Sequential\"\n",
        "\n",
        "model = Sequential() #define your own model\n",
        "for layer in vgg19_model.layers[:-1]: # elimilate the last 'Dense' layer because we don't want 1000 classes, only 5 classes\n",
        "    model.add(layer)\n",
        "model.summary() # to see the last layer is gone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "ualQo9rSTV-c"
      },
      "outputs": [],
      "source": [
        "for layer in model.layers:\n",
        "   layer.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VKEucXSTgNv",
        "outputId": "22376824-67ec-4242-cc16-726c1c07be25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv4 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv4 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv4 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 25088)             0         \n",
            "                                                                 \n",
            " fc1 (Dense)                 (None, 4096)              102764544 \n",
            "                                                                 \n",
            " fc2 (Dense)                 (None, 4096)              16781312  \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 5)                 20485     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 139,590,725\n",
            "Trainable params: 20,485\n",
            "Non-trainable params: 139,570,240\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Adding a softmax layer at the end to output as one of the 5 classes\n",
        "model.add(Dense(5,activation='softmax')) # total 5 classes.\n",
        "model.summary() # to see we've changed the last layer to only output 4 classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLnBHh4iT9IX"
      },
      "source": [
        "### Training the customized model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "Co7z-2MvUCef"
      },
      "outputs": [],
      "source": [
        "model.compile(tf.keras.optimizers.Adam(learning_rate=0.00075), loss='categorical_crossentropy', metrics=['accuracy']) # define loss function and loss optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#steps_per_epoch = TotalTrainingSamples / TrainingBatchSize\n",
        "steps_per_epoch = len(train_batches)\n",
        "print(steps_per_epoch)\n",
        "\n",
        "#validation_steps = total_validation_samples // validation_batch_size\n",
        "validation_steps = len(valid_batches)\n",
        "print(validation_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVbWjeIhUvIK",
        "outputId": "f08221b8-ff9c-476b-b4a8-00b9d352b9e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "89/89 - 132s - loss: 0.3515 - accuracy: 0.8789 - val_loss: 0.0903 - val_accuracy: 0.9793 - 132s/epoch - 1s/step\n",
            "Epoch 2/3\n",
            "89/89 - 133s - loss: 0.0196 - accuracy: 0.9972 - val_loss: 0.0916 - val_accuracy: 0.9711 - 133s/epoch - 1s/step\n",
            "Epoch 3/3\n",
            "89/89 - 135s - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.0664 - val_accuracy: 0.9876 - 135s/epoch - 2s/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x15aa65670>"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(train_batches, steps_per_epoch=steps_per_epoch,validation_data=valid_batches,validation_steps=validation_steps,epochs=3,verbose=2) # train process\n",
        "# train your model using all 'train' pictures, and test it with all 'valid' pictures, so you can see there are two 'acc' below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTXywum-WF6c"
      },
      "source": [
        "### Using the trained model on testing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "nMEqSIU7WMa0",
        "outputId": "4bea17a5-39c1-4ed8-e060-e88d5cc6ba81"
      },
      "outputs": [],
      "source": [
        "test_imgs,test_labels = next(test_batches) # show some test pictures\n",
        "plots(test_imgs,titles=test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "7WQrIaB8XIPg"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(test_batches,steps=5,verbose=0) # predict the class of each picture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypraxs2YXJDI",
        "outputId": "bd47bac6-5565-4df1-9b21-c11b27cc0ea0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[2.30013624e-01, 3.08254559e-04, 2.58847256e-04, 4.03437807e-05,\n",
              "        7.69378960e-01],\n",
              "       [4.99410719e-01, 6.71279877e-02, 3.55289690e-02, 8.79549328e-03,\n",
              "        3.89136791e-01],\n",
              "       [1.01450769e-05, 1.35197911e-06, 3.08362792e-06, 1.04531200e-05,\n",
              "        9.99974966e-01],\n",
              "       [1.28567219e-03, 1.74653751e-03, 2.44182185e-04, 9.96719539e-01,\n",
              "        4.05852688e-06],\n",
              "       [1.10164843e-03, 7.72221210e-06, 7.59168006e-06, 2.64709088e-04,\n",
              "        9.98618364e-01],\n",
              "       [9.99335349e-01, 4.03311533e-05, 2.08686452e-05, 4.73774271e-04,\n",
              "        1.29733715e-04],\n",
              "       [5.03196614e-04, 2.85314309e-04, 1.87886799e-05, 9.99191105e-01,\n",
              "        1.65120480e-06],\n",
              "       [1.06592877e-02, 3.38986237e-03, 9.84532058e-01, 1.30176102e-03,\n",
              "        1.16963573e-04],\n",
              "       [5.09105250e-03, 9.61691380e-01, 1.79853681e-02, 1.13835270e-02,\n",
              "        3.84871382e-03],\n",
              "       [9.04744525e-07, 9.94089484e-01, 8.68986081e-06, 5.81987947e-03,\n",
              "        8.11020946e-05],\n",
              "       [4.37663373e-04, 7.40449468e-04, 1.24229270e-03, 9.97572720e-01,\n",
              "        6.87469628e-06],\n",
              "       [1.69822690e-03, 1.23384481e-04, 1.46874285e-04, 2.54756026e-03,\n",
              "        9.95483994e-01],\n",
              "       [1.01464238e-05, 1.92719555e-04, 9.99748290e-01, 4.82726537e-05,\n",
              "        5.44665170e-07],\n",
              "       [5.32256216e-02, 1.71276987e-01, 8.07302743e-02, 6.81236625e-01,\n",
              "        1.35304043e-02],\n",
              "       [9.38746234e-05, 9.10950708e-04, 9.98576760e-01, 4.13850765e-04,\n",
              "        4.57629039e-06],\n",
              "       [1.88175545e-05, 1.84926917e-04, 1.45560000e-06, 9.99791801e-01,\n",
              "        2.99416638e-06],\n",
              "       [2.36815456e-02, 1.57125539e-03, 9.73199069e-01, 1.41432206e-03,\n",
              "        1.33806316e-04],\n",
              "       [2.58317377e-05, 1.49962216e-04, 9.99782503e-01, 3.82532562e-05,\n",
              "        3.49489505e-06],\n",
              "       [2.60000117e-04, 9.98992503e-01, 1.11827656e-04, 6.34524506e-04,\n",
              "        1.24983421e-06],\n",
              "       [5.24130228e-05, 9.59598692e-04, 1.17162826e-05, 9.98974562e-01,\n",
              "        1.64607081e-06],\n",
              "       [9.98767018e-01, 5.56230021e-04, 1.30814384e-04, 5.32567674e-06,\n",
              "        5.40603301e-04],\n",
              "       [3.53377289e-03, 1.41871267e-03, 9.94328320e-01, 4.61430056e-04,\n",
              "        2.57637468e-04],\n",
              "       [3.01762993e-05, 7.86059616e-07, 4.90512048e-06, 2.01291823e-05,\n",
              "        9.99943972e-01],\n",
              "       [5.92160859e-06, 2.25501225e-07, 1.33997830e-07, 4.49739900e-07,\n",
              "        9.99993324e-01],\n",
              "       [4.32462111e-04, 9.90473807e-01, 7.80252667e-05, 8.99764989e-03,\n",
              "        1.80764746e-05]], dtype=float32)"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions \n",
        "# print out the predicted label value, also in 'one-hot' format\n",
        "# compare the prediction with the true label of pictures, you can see they are very close"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wY9-EW-XUQU",
        "outputId": "fc666619-b12a-488c-955a-3219f5db4742"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The image prediction of image 0 is: Stop Moving\n",
            "The image prediction of image 1 is: Go Forward\n",
            "The image prediction of image 2 is: Stop Moving\n",
            "The image prediction of image 3 is: Turn Right\n",
            "The image prediction of image 4 is: Stop Moving\n",
            "The image prediction of image 5 is: Go Forward\n",
            "The image prediction of image 6 is: Turn Right\n",
            "The image prediction of image 7 is: Reverse\n",
            "The image prediction of image 8 is: Turn Left\n",
            "The image prediction of image 9 is: Turn Left\n",
            "The image prediction of image 10 is: Turn Right\n",
            "The image prediction of image 11 is: Stop Moving\n",
            "The image prediction of image 12 is: Reverse\n",
            "The image prediction of image 13 is: Turn Right\n",
            "The image prediction of image 14 is: Reverse\n",
            "The image prediction of image 15 is: Turn Right\n",
            "The image prediction of image 16 is: Reverse\n",
            "The image prediction of image 17 is: Reverse\n",
            "The image prediction of image 18 is: Turn Left\n",
            "The image prediction of image 19 is: Turn Right\n",
            "The image prediction of image 20 is: Go Forward\n",
            "The image prediction of image 21 is: Reverse\n",
            "The image prediction of image 22 is: Stop Moving\n",
            "The image prediction of image 23 is: Stop Moving\n",
            "The image prediction of image 24 is: Turn Left\n"
          ]
        }
      ],
      "source": [
        "for i, prediction in enumerate(predictions):\n",
        "  if max(prediction) == prediction[0]:\n",
        "    print(\"The image prediction of image \" + str(i) + \" is: Go Forward\")\n",
        "  if max(prediction) == prediction[1]:\n",
        "    print(\"The image prediction of image \" + str(i) + \" is: Turn Left\")\n",
        "  if max(prediction) == prediction[2]:\n",
        "    print(\"The image prediction of image \" + str(i) + \" is: Reverse\")\n",
        "  if max(prediction) == prediction[3]:\n",
        "    print(\"The image prediction of image \" + str(i) + \" is: Turn Right\")\n",
        "  if max(prediction) == prediction[4]:\n",
        "    print(\"The image prediction of image \" + str(i) + \" is: Stop Moving\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5oRtQq8M7s_",
        "outputId": "fbbabf7c-e147-4463-8c46-494068ff8e87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "49/49 - 35s - loss: 0.0598 - accuracy: 0.9878 - 35s/epoch - 708ms/step\n",
            "Test loss: 0.05981171131134033\n",
            "Test accuracy: 0.9877551198005676\n"
          ]
        }
      ],
      "source": [
        "#Get test accuracy\n",
        "score = model.evaluate(test_batches, verbose=2) \n",
        "print('Test loss:', score[0]) \n",
        "print('Test accuracy:', score[1]) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwQY4kmpIw5e"
      },
      "source": [
        "### Save your trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yM7mpaQI4dJ",
        "outputId": "2ee30b6b-1593-4b82-df14-39c780bb5cf9"
      },
      "outputs": [],
      "source": [
        "model.save('model.second')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APc9eqEhNdT6"
      },
      "source": [
        "### Load your trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "ic6NqQOyNhzB",
        "outputId": "e8cf331e-feb3-4999-b28d-d8728b8cd182"
      },
      "outputs": [],
      "source": [
        "model = load_model('model.second')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWYph9NXQ6wB"
      },
      "source": [
        "### User Interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[8.4267694e-01 8.5547874e-03 9.0304911e-03 1.3954391e-01 1.9390338e-04]\n",
            "0\n",
            "Go Forward\n",
            "[0.00238338 0.13517465 0.00785785 0.8480503  0.00653388]\n",
            "3\n",
            "Turn Right\n",
            "[0.00157537 0.12287932 0.0224736  0.8394319  0.01363983]\n",
            "3\n",
            "Turn Right\n",
            "[0.00175337 0.07627181 0.03289405 0.87574273 0.01333818]\n",
            "3\n",
            "Turn Right\n",
            "[2.6451580e-03 3.4290012e-02 3.7781745e-02 9.2454833e-01 7.3475175e-04]\n",
            "3\n",
            "Turn Right\n",
            "[2.3548983e-03 7.6766582e-03 2.7182695e-02 9.6236795e-01 4.1783464e-04]\n",
            "3\n",
            "Turn Right\n",
            "[0.00821583 0.02160033 0.07159013 0.89763707 0.00095652]\n",
            "3\n",
            "Turn Right\n",
            "[0.00567194 0.0161178  0.0795614  0.8971774  0.00147143]\n",
            "3\n",
            "Turn Right\n",
            "[5.2833660e-03 9.5983772e-03 6.0294267e-02 9.2402923e-01 7.9469976e-04]\n",
            "3\n",
            "Turn Right\n",
            "[0.00334028 0.01167113 0.06603698 0.9178376  0.00111392]\n",
            "3\n",
            "Turn Right\n",
            "[0.00269984 0.13297182 0.8055911  0.03891506 0.01982231]\n",
            "2\n",
            "Reverse\n",
            "[0.0301873  0.02473557 0.8959796  0.02671736 0.02238014]\n",
            "2\n",
            "Reverse\n",
            "[4.6279153e-04 9.4374630e-04 7.2836981e-04 1.6029922e-03 9.9626213e-01]\n",
            "4\n",
            "Stop Moving\n",
            "[1.8615642e-04 4.2172259e-04 7.6980720e-04 2.6119032e-04 9.9836117e-01]\n",
            "4\n",
            "Stop Moving\n",
            "[1.57051792e-04 1.03436694e-04 3.90970294e-04 2.34036837e-04\n",
            " 9.99114454e-01]\n",
            "4\n",
            "Stop Moving\n",
            "[6.5842923e-04 1.9659549e-04 1.9781012e-03 4.0928368e-04 9.9675757e-01]\n",
            "4\n",
            "Stop Moving\n",
            "[0.0046277  0.00182266 0.01372579 0.01038676 0.9694371 ]\n",
            "4\n",
            "Stop Moving\n"
          ]
        }
      ],
      "source": [
        "#Fixed image dimensions\n",
        "width = 224\n",
        "height = 224\n",
        "dim = (width, height)\n",
        "\n",
        "# Set up for fps counter\n",
        "fpsCounter = cvzone.FPS()\n",
        "\n",
        "# Start video stream from your computer camera\n",
        "capture = cv2.VideoCapture(0) \n",
        "\n",
        "# Using cvzone library to detect and track the hand\n",
        "detector = HandDetector(detectionCon=0.8, minTrackCon=0.8, maxHands=2)\n",
        "\n",
        "# Capture continuous video\n",
        "i = 1 \n",
        "while True:\n",
        "    # Get image frame\n",
        "    ret, image = capture.read() # read from the video\n",
        "\n",
        "    # Get interruption key from keyboard\n",
        "    k = cv2.waitKey(1) # get input from keyboard\n",
        "\n",
        "    # Flip image frame\n",
        "    cv2image1 = cv2.flip(image,1)\n",
        "    #cv2image2 = cv2.cvtColor(cv2image1, cv2.COLOR_BGR2RGBA) # convert video to RGB format  \n",
        "    \n",
        "    # Show FPS on screen\n",
        "    fps, img = fpsCounter.update(cv2image1,pos=(50,80),color=(0,255,0),scale=3,thickness=3)\n",
        "\n",
        "    # Find the hand and its landmarks\n",
        "    hands = detector.findHands(img, draw=False, flipType=False)\n",
        "\n",
        "    #print(img)\n",
        "    \n",
        "    rightHand = 0\n",
        "\n",
        "    if hands:\n",
        "        # Hand 1\n",
        "        hand1 = hands[0]\n",
        "        lmList1 = hand1[\"lmList\"]  # List of 21 Landmark points\n",
        "        bbox1 = hand1[\"bbox\"]  # Bounding box info x,y,w,h\n",
        "        centerPoint1 = hand1['center']  # center of the hand cx,cy\n",
        "        handType1 = hand1[\"type\"]  # Handtype Left or Right\n",
        "        \n",
        "        if handType1 == \"Right\":\n",
        "            rightHand = hand1\n",
        "            rightbbox = rightHand[\"bbox\"]  # Bounding box info x,y,w,h \n",
        "        \n",
        "        #fingers1 = detector.fingersUp(hand1)\n",
        "\n",
        "        if len(hands) == 2:\n",
        "            # Hand 2\n",
        "            hand2 = hands[1]\n",
        "            lmList2 = hand2[\"lmList\"]  # List of 21 Landmark points\n",
        "            bbox2 = hand2[\"bbox\"]  # Bounding box info x,y,w,h\n",
        "            centerPoint2 = hand2['center']  # center of the hand cx,cy\n",
        "            handType2 = hand2[\"type\"]  # Hand Type \"Left\" or \"Right\"\n",
        "\n",
        "            if handType2 == \"Right\":\n",
        "                rightHand = hand2 \n",
        "                rightbbox = rightHand[\"bbox\"]  # Bounding box info x,y,w,h \n",
        "            \n",
        "            #fingers2 = detector.fingersUp(hand2)\n",
        "\n",
        "    \n",
        "    cv2.rectangle(img,(460,160),(960,660),(0,255,0),5) # draw square\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    cv2.putText(img,'Put your RIGHT hand in the square',(465,140),font,0.8,(0,255,0),2,cv2.LINE_AA) \n",
        "    #cv2.putText(cv2image1,'Put your RIGHT hand in this square!',(300,40),font,0.6,(0,255,0),2,cv2.LINE_AA)\n",
        "    cv2.imshow('Capturing live feed',img)\n",
        "    #cv2.imshow('Taking data pictures',img)\n",
        "\n",
        "    if rightHand !=0:\n",
        "        cv2image = cv2image1[163:657, 463:957] # Take the image within the square\n",
        "        crop_cv2image = cv2.resize(cv2image, dim, interpolation = cv2.INTER_AREA)\n",
        "        img = crop_cv2image.reshape((1,)+ crop_cv2image.shape ) #reshape it so that model can accept\n",
        "\n",
        "        cv2.imshow('The picture 1',cv2image)\n",
        "        cv2.imshow('The picture 2',crop_cv2image)\n",
        "\n",
        "        prediction = model.predict(img)[0]    \n",
        "        \n",
        "        pred_class = list(prediction).index(max(prediction))\n",
        "\n",
        "        rightHandFingers = detector.fingersUp(rightHand)\n",
        "\n",
        "        #if rightHandFingers[1] and rightHandFingers[2]:\n",
        "        if max(prediction) > 0.8:\n",
        "            print(prediction) \n",
        "            print(pred_class)\n",
        "            \n",
        "            if pred_class==0:\n",
        "                print('Go Forward')\n",
        "                #command(\"fwd_bit\")\n",
        "            elif pred_class==1:\n",
        "                print('Turn Left')\n",
        "                #command(\"left_bit\")\n",
        "            elif pred_class==2:\n",
        "                print('Reverse')\n",
        "                #command(\"left_bit\")\n",
        "            elif pred_class==3:\n",
        "                print('Turn Right')\n",
        "                #command(\"right_bit\")\n",
        "            elif pred_class==4:\n",
        "                print('Stop Moving')  \n",
        "                #command(\"stop\")\n",
        "        # else:\n",
        "        #     print('Stop Moving')\n",
        "          \n",
        "    if k ==27: # press 'Esc' key to exit \n",
        "        break # exit while loop\n",
        "        \n",
        "#========================================\n",
        "\n",
        "cv2.destroyAllWindows() # close down window\n",
        "capture.release() # disable your camera  "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Space-Rover-Mission.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
